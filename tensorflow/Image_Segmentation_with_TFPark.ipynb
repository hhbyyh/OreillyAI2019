{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Image Segmentation with TFPark",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBBCrXVaccah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env SPARK_DRIVER_MEMORY=10g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ULKtInm7dAMK"
      },
      "source": [
        "# Image Segmentation with `tf.keras` and TFPark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cl79rk4KKol8"
      },
      "source": [
        "Note: This tutorial is ported from https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb\n",
        "\n",
        "In this tutorial we will learn how to segment images. **Segmentation** is the process of generating pixel-wise segmentations giving the class of the object visible at each pixel. For example, we could be identifying the location and boundaries of people within an image or identifying cell nuclei from an image. Formally, image segmentation refers to the process of partitioning an image into a set of pixels that we desire to identify (our target) and the background. \n",
        "\n",
        "Specifically, in this tutorial we will be using the [Kaggle Carvana Image Masking Challenge Dataset](https://www.kaggle.com/c/carvana-image-masking-challenge). \n",
        "\n",
        "This dataset contains a large number of car images, with each car taken from different angles. In addition, for each car image, we have an associated manually cutout mask; our task will be to automatically create these cutout masks for unseen data. \n",
        "\n",
        "## Specific concepts that will be covered:\n",
        "In the process, we will build practical experience and develop intuition around the following concepts:\n",
        "* **[Functional API](https://keras.io/getting-started/functional-api-guide/)** - we will be implementing UNet, a convolutional network model classically used for biomedical image segmentation with the Functional API. \n",
        "  * This model has layers that require multiple input/outputs. This requires the use of the functional API\n",
        "  * Check out the original [paper](https://arxiv.org/abs/1505.04597), \n",
        "U-Net: Convolutional Networks for Biomedical Image Segmentation by Olaf Ronneberger!\n",
        "* **Custom Loss Functions and Metrics** - We'll implement a custom loss function using binary [**cross entropy**](https://developers.google.com/machine-learning/glossary/#cross-entropy) and **dice loss**. We'll also implement **dice coefficient** (which is used for our loss) and **mean intersection over union**, that will help us monitor our training process and judge how well we are performing.  \n",
        "\n",
        "### We will follow the general workflow:\n",
        "1. Visualize data/perform some exploratory data analysis\n",
        "2. Set up data pipeline and preprocessing\n",
        "3. Build model\n",
        "4. Train model\n",
        "5. Evaluate model\n",
        "6. Repeat\n",
        "\n",
        "**Audience:** This post is geared towards intermediate users who are comfortable with basic machine learning concepts.\n",
        "\n",
        "**Time Estimated**: 60 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-VnVyvljx1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install analytics-zoo\n",
        "!pip install analytics-zoo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trvGZPRYl_C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C92ucfQz0BIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /usr/lib/jvm/java-8-openjdk-amd64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g32hJxI1mJcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_WH5ReHyot8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ODNLPGHKKgr-",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQ9VRReUQxXi",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend as K\n",
        "from zoo.common.nncontext import * \n",
        "\n",
        "sc = init_nncontext(\"image segmentation\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RW9gk331S0KA"
      },
      "source": [
        "# Get all the files \n",
        "You **should manually download** the dataset from kaggle [carvana-image-masking-challenge](https://www.kaggle.com/c/carvana-image-masking-challenge/data) and put it to the following path. We will need three files, `train.zip`, `train_mask.zip` and `train_mask.csv.zip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sCabxCwnqGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://ai-conf-sj-2019.s3-us-west-2.amazonaws.com/carvana.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW0Mh1GmoL3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip carvana.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MOTOyU3dAMU",
        "colab": {}
      },
      "source": [
        "file_path = 'carvana/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3gJSCmWjdAMW",
        "colab": {}
      },
      "source": [
        "# Download data from Kaggle and unzip the files of interest. \n",
        "def load_data_from_zip(file_path, file):\n",
        "    with zipfile.ZipFile(os.path.join(file_path, file), \"r\") as zip_ref:\n",
        "        unzipped_file = zip_ref.namelist()[0]\n",
        "        zip_ref.extractall(file_path)\n",
        "\n",
        "def load_data(file_path):\n",
        "    load_data_from_zip(file_path, 'train.zip')\n",
        "    load_data_from_zip(file_path, 'train_masks.zip')\n",
        "    load_data_from_zip(file_path, 'train_masks.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_SsQjuN2dWmU",
        "colab": {}
      },
      "source": [
        "load_data(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsNA79V4oeo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls carvana"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wT1kb3q0ghhi",
        "colab": {}
      },
      "source": [
        "img_dir = os.path.join(file_path, \"train\")\n",
        "label_dir = os.path.join(file_path, \"train_masks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSJslv6ccbE",
        "colab_type": "text"
      },
      "source": [
        "Here we only takes the first 1000 files for simplicity. You can use all images if you have enough computation resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ej-e6cqmRgd",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(os.path.join(file_path, 'train_masks.csv'))\n",
        "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
        "ids_train = ids_train[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "33i4xFXweztH",
        "colab": {}
      },
      "source": [
        "x_train_filenames = []\n",
        "y_train_filenames = []\n",
        "for img_id in ids_train:\n",
        "    x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
        "    y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DtutNudKbf70",
        "colab": {}
      },
      "source": [
        "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
        "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zDycQekHaMqq",
        "colab": {}
      },
      "source": [
        "num_train_examples = len(x_train_filenames)\n",
        "num_val_examples = len(x_val_filenames)\n",
        "\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of validation examples: {}\".format(num_val_examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nhda5fkPS3JD"
      },
      "source": [
        "### Here's what the paths look like "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Di1N83ArilzR",
        "colab": {}
      },
      "source": [
        "x_train_filenames[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gc-BDv1Zio1z",
        "colab": {}
      },
      "source": [
        "y_train_filenames[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhvDoZkbcUa1"
      },
      "source": [
        "# Visualize\n",
        "Let's take a look at some of the examples of different images in our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qUA6SDLhozjj",
        "colab": {}
      },
      "source": [
        "display_num = 5\n",
        "\n",
        "r_choices = np.random.choice(num_train_examples, display_num)\n",
        "\n",
        "plt.figure(figsize=(10, 15))\n",
        "for i in range(0, display_num * 2, 2):\n",
        "    img_num = r_choices[i // 2]\n",
        "    x_pathname = x_train_filenames[img_num]\n",
        "    y_pathname = y_train_filenames[img_num]\n",
        "  \n",
        "    plt.subplot(display_num, 2, i + 1)\n",
        "    plt.imshow(mpimg.imread(x_pathname))\n",
        "    plt.title(\"Original Image\")\n",
        "  \n",
        "    example_labels = Image.open(y_pathname)\n",
        "    label_vals = np.unique(example_labels)\n",
        "  \n",
        "    plt.subplot(display_num, 2, i + 2)\n",
        "    plt.imshow(example_labels)\n",
        "    plt.title(\"Masked Image\")  \n",
        "  \n",
        "plt.suptitle(\"Examples of Images and their Masks\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d4CPgvPiToB_"
      },
      "source": [
        "# Set up "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HfeMRgyoa2n6"
      },
      "source": [
        "Let’s begin by setting up some parameters. We’ll standardize and resize all the shapes of the images. We’ll also set up some training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeDoiSFlothe",
        "colab": {}
      },
      "source": [
        "img_shape = (128, 128, 3)\n",
        "batch_size = 20\n",
        "epochs = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8_d5ATP21npW"
      },
      "source": [
        "Here we resize the image to 128 x 128 to reduce the running time and memory overhead. You can can get better result by using higher resolution images. Also, it is important to note that due to the architecture of our UNet version, the size of the image must be evenly divisible by a factor of 32, as we down sample the spatial resolution by a factor of 2 with each `MaxPooling2Dlayer`.\n",
        "\n",
        "\n",
        "If your machine can support it, you will achieve better performance using a higher resolution input image (e.g. 512 by 512) as this will allow more precise localization and less loss of information during encoding. In addition, you can also make the model deeper.\n",
        "\n",
        "\n",
        "Alternatively, if your machine cannot support it, lower the image resolution and/or batch size. Note that lowering the image resolution will decrease performance and lowering batch size will increase training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_HONB9JbXxDM"
      },
      "source": [
        "# Build our input pipeline with PySpark and `TFDataset`\n",
        "Since we begin with filenames, we will need to build a robust and scalable data pipeline that will play nicely with our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3aGi28u8Cq9M"
      },
      "source": [
        "## Processing each pathname"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC6ezrrTccbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import misc\n",
        "def load_and_process_image(file_path):\n",
        "    array = mpimg.imread(file_path)\n",
        "    result = np.array(Image.fromarray(array).resize((128, 128)))\n",
        "    result = result.astype(float)\n",
        "    result /= 255.0\n",
        "    return result\n",
        "\n",
        "def load_and_process_image_label(file_path):\n",
        "    array = mpimg.imread(file_path)\n",
        "    result = np.array(Image.fromarray(array).resize((128, 128)))\n",
        "    result = np.expand_dims(result[:, :, 1], axis=-1)\n",
        "    result = result.astype(float)\n",
        "    result /= 255.0\n",
        "    return result\n",
        "    \n",
        "train_images = sc.parallelize(x_train_filenames).map(lambda filepath: load_and_process_image(filepath))\n",
        "train_label_images = sc.parallelize(y_train_filenames).map(lambda filepath: load_and_process_image_label(filepath))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuCVSUHfccba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_images = np.stack([load_and_process_image(filepath) for filepath in x_val_filenames])\n",
        "val_label_images = np.stack([load_and_process_image_label(filepath) for filepath in y_val_filenames])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICKebE5Bccbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_images.shape, val_label_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUbD13WBccbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zoo.tfpark import *\n",
        "train_rdd = train_images.zip(train_label_images)\n",
        "\n",
        "dataset = TFDataset.from_rdd(train_rdd,\n",
        "                             features=(tf.float32, [128, 128, 3]),\n",
        "                             labels=(tf.float32, [128, 128, 1]),\n",
        "                             batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvtxCncKsoRd"
      },
      "source": [
        "# Build the model\n",
        "We'll build the U-Net model. U-Net is especially good with segmentation tasks because it can localize well to provide high resolution segmentation masks. In addition, it works well with small datasets and is relatively robust against overfitting as the training data is in terms of the number of patches within an image, which is much larger than the number of training images itself. Unlike the original model, we will add batch normalization to each of our blocks. \n",
        "\n",
        "The Unet is built with an encoder portion and a decoder portion. The encoder portion is composed of a linear stack of [`Conv`](https://developers.google.com/machine-learning/glossary/#convolution), `BatchNorm`, and [`Relu`](https://developers.google.com/machine-learning/glossary/#ReLU) operations followed by a [`MaxPool`](https://developers.google.com/machine-learning/glossary/#pooling). Each `MaxPool` will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high resolution feature maps with the decoder portion. The Decoder portion is comprised of UpSampling2D, Conv, BatchNorm, and Relus. Note that we concatenate the feature map of the same size on the decoder side. Finally, we add a final Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale. \n",
        "## The Keras Functional API\n",
        "The Keras functional API is used when you have multi-input/output models, shared layers, etc. It's a powerful API that allows you to manipulate tensors and build complex graphs with intertwined datastreams easily. In addition it makes **layers** and **models** both callable on tensors. \n",
        "  * To see more examples check out the [get started guide](https://keras.io/getting-started/functional-api-guide/). \n",
        "  \n",
        "  \n",
        "  We'll build these helper functions that will allow us to ensemble our model block operations easily and simply. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zfew1i1F6bK-",
        "colab": {}
      },
      "source": [
        "def conv_block(input_tensor, num_filters):\n",
        "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "    encoder = layers.Activation('relu')(encoder)\n",
        "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "    encoder = layers.Activation('relu')(encoder)\n",
        "    return encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "    encoder = conv_block(input_tensor, num_filters)\n",
        "    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "  \n",
        "    return encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "    decoder = layers.Activation('relu')(decoder)\n",
        "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "    decoder = layers.Activation('relu')(decoder)\n",
        "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "    decoder = layers.Activation('relu')(decoder)\n",
        "    return decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRLp21S_hpTn",
        "colab": {}
      },
      "source": [
        "inputs = layers.Input(shape=(128, 128, 3))\n",
        "# 128\n",
        "\n",
        "encoder0_pool, encoder0 = encoder_block(inputs, 16)\n",
        "# 64\n",
        "\n",
        "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 32)\n",
        "# 32\n",
        "\n",
        "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 64)\n",
        "# 16\n",
        "\n",
        "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 128)\n",
        "# 8\n",
        "\n",
        "center = conv_block(encoder3_pool, 256)\n",
        "# center\n",
        "\n",
        "decoder3 = decoder_block(center, encoder3, 128)\n",
        "# 16\n",
        "\n",
        "decoder2 = decoder_block(decoder3, encoder2, 64)\n",
        "# 32\n",
        "\n",
        "decoder1 = decoder_block(decoder2, encoder1, 32)\n",
        "# 64\n",
        "\n",
        "decoder0 = decoder_block(decoder1, encoder0, 16)\n",
        "# 128\n",
        "\n",
        "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "luDqDqu8c1AX"
      },
      "source": [
        "## Define your model\n",
        "Using functional API, you must define your model by specifying the inputs and outputs associated with the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76QkTzXVczgc",
        "colab": {}
      },
      "source": [
        "net = models.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p0tNnmyOdtyr"
      },
      "source": [
        "# Defining custom metrics and loss functions\n",
        "Defining loss and metric functions are simple with Keras. Simply define a function that takes both the True labels for a given example and the Predicted labels for the same given example. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sfuBVut0fogM"
      },
      "source": [
        "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced. \n",
        "\n",
        "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t_8_hbHECUAW",
        "colab": {}
      },
      "source": [
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    # Flatten\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DgINhlpNaxP",
        "colab": {}
      },
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qqClGNFJdANU"
      },
      "source": [
        "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "udrfi9JGB-bL",
        "colab": {}
      },
      "source": [
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LifmpjXNc9Gz"
      },
      "source": [
        "## Compile your model\n",
        "We use our custom loss function to minimize. In addition, we specify what metrics we want to keep track of as we train. Note that metrics are not actually used during the training process to tune the parameters, but are instead used to measure performance of the training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gflcWk2Cc8Bi",
        "colab": {}
      },
      "source": [
        "net.compile(optimizer=tf.keras.optimizers.Adam(2e-3), loss=bce_dice_loss)\n",
        "\n",
        "net.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8WG_8iZ_dMbK"
      },
      "source": [
        "## Train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUnMVQZ8cccC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_model = KerasModel(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UMZcOrq5aaj1",
        "colab": {}
      },
      "source": [
        "keras_model.fit(dataset, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MGFKf8yCTYbw"
      },
      "source": [
        "# Visualize actual performance \n",
        "We'll visualize our performance on the validation set.\n",
        "\n",
        "Note that in an actual setting (competition, deployment, etc.) we'd evaluate on the test set with the full image resolution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTcDkTTucccJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Running next element in our graph will produce a batch of images\n",
        "plt.figure(figsize=(10, 20))\n",
        "for i in range(5):\n",
        "    img, label = val_images[i], val_label_images[i]\n",
        "    batch_img = np.expand_dims(img, axis=0)\n",
        "    predicted_label = keras_model.predict(batch_img)[0]\n",
        "\n",
        "    plt.subplot(5, 3, 3 * i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Input image\")\n",
        "  \n",
        "    plt.subplot(5, 3, 3 * i + 2)\n",
        "    plt.imshow(label[:, :, 0], cmap='gray')\n",
        "    plt.title(\"Actual Mask\")\n",
        "    plt.subplot(5, 3, 3 * i + 3)\n",
        "    plt.imshow(predicted_label[:, :, 0], cmap='gray')\n",
        "    plt.title(\"Predicted Mask\")\n",
        "plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}